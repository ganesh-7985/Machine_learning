## Notes
- Tune via env vars: `PHASE1_EPOCHS`, `BATCH_SIZE`, `VAL_RATIO`, `LR`, `WEIGHT_DECAY`, `WARMUP_EPOCHS`, `PL_THRESHOLD`, `PL_MAX_RATIO`, `PL_EPOCHS`.
- Outputs saved to `DATASET_ROOT`: `phase1_predictions.csv`, `phase2_predictions.csv`.
- Keep to 30-min budget by reducing epochs/batch size.
# @title Phase 2: Finetune with Pseudo-Labels and Export Predictions
cfg = PseudoLabelingConfig(
    confidence_threshold=float(os.environ.get('PL_THRESHOLD', 0.9)),
    max_unlabeled_ratio=float(os.environ.get('PL_MAX_RATIO', 1.0)),
    epochs_finetune=int(os.environ.get('PL_EPOCHS', 10)),
    batch_size=BATCH_SIZE,
    lr=LR,
    weight_decay=WEIGHT_DECAY,
    warmup_epochs=WARMUP_EPOCHS,
    val_ratio=VAL_RATIO,
    num_workers=NUM_WORKERS,
)

PHASE2_CKPT = str(Path(DATASET_ROOT) / 'phase2_resnet18.pth')
phase2_ckpt_path = train_with_pseudo(DATASET_DIR, PHASE1_CKPT, cfg, PHASE2_CKPT)

model_ssl = create_resnet18(NUM_CLASSES, pretrained=False).to(DEVICE)
ckpt2 = torch.load(phase2_ckpt_path, map_location=DEVICE)
model_ssl.load_state_dict(ckpt2['model_state'])
model_ssl.eval()

_, _, _, _, test_loader = build_dataloaders(DATASET_DIR, batch_size=BATCH_SIZE, val_ratio=VAL_RATIO, num_workers=NUM_WORKERS)
rows2 = []
with torch.no_grad():
    for images, fnames in test_loader:
        images = images.to(DEVICE, non_blocking=True)
        preds = model_ssl(images).argmax(dim=1).cpu().tolist()
        for fname, pred in zip(fnames, preds):
            rows2.append({'path': fname, 'predicted_label': f'class_{pred}'})
phase2_csv = str(Path(DATASET_ROOT) / 'phase2_predictions.csv')
pd.DataFrame(rows2).to_csv(phase2_csv, index=False)
print('Saved:', phase2_csv)
# @title Semi-Supervised: Pseudo-Labeling
from dataclasses import dataclass

@dataclass
class PseudoLabelingConfig:
    confidence_threshold: float = 0.9
    max_unlabeled_ratio: float = 1.0
    epochs_finetune: int = 10
    batch_size: int = 64
    lr: float = 1e-3
    weight_decay: float = 1e-4
    warmup_epochs: int = 1
    val_ratio: float = 0.1
    num_workers: int = 2


def generate_pseudo_labels(model: nn.Module, unlabeled_loader: DataLoader, threshold: float = 0.9):
    model.eval()
    pseudo_items: List[Tuple[str, int]] = []
    softmax = nn.Softmax(dim=1)
    with torch.no_grad():
        for images, fnames in unlabeled_loader:
            images = images.to(DEVICE, non_blocking=True)
            probs = softmax(model(images))
            confs, preds = probs.max(dim=1)
            for fname, conf, pred in zip(fnames, confs.cpu().tolist(), preds.cpu().tolist()):
                if conf >= threshold:
                    pseudo_items.append((fname, pred))
    return pseudo_items

class PseudoLabeledDataset(Dataset):
    def __init__(self, root: str, items: List[Tuple[str, int]], transform=None):
        self.root = Path(root)
        self.items = items
        self.transform = transform
    def __len__(self):
        return len(self.items)
    def __getitem__(self, idx: int):
        fname, label = self.items[idx]
        with Image.open(self.root / fname).convert('RGB') as img:
            if self.transform is not None:
                img = self.transform(img)
        return img, label


def combine_labeled_and_pseudo(dataset_dir: str, pseudo_items: List[Tuple[str, int]], max_unlabeled_ratio: float = 1.0, batch_size: int = 64, val_ratio: float = 0.1, num_workers: int = 2):
    labeled_root = str(Path(dataset_dir) / 'labeled_data')
    labeled_csv = str(Path(labeled_root) / 'labels.csv')
    ds_labeled = ImageDatasetWithLabels(labeled_root, labeled_csv, transform=train_tfms)
    pseudo_items = pseudo_items[: int(len(ds_labeled) * max_unlabeled_ratio) ]
    ds_pseudo = PseudoLabeledDataset(str(Path(dataset_dir)/'unlabeled_data'), pseudo_items, transform=train_tfms)
    ds_combined = torch.utils.data.ConcatDataset([ds_labeled, ds_pseudo])

    val_size = max(1, int(len(ds_labeled) * val_ratio))
    train_size = len(ds_combined) - val_size
    ds_train, ds_val = random_split(ds_combined, [train_size, val_size])

    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return train_loader, val_loader


def train_with_pseudo(dataset_dir: str, base_ckpt_path: str, cfg: PseudoLabelingConfig, out_ckpt_path: str):
    base = create_resnet18(NUM_CLASSES, pretrained=False).to(DEVICE)
    ckpt = torch.load(base_ckpt_path, map_location=DEVICE)
    base.load_state_dict(ckpt['model_state'])
    base.eval()

    _, _, _, unlabeled_loader, _ = build_dataloaders(dataset_dir, batch_size=cfg.batch_size, val_ratio=cfg.val_ratio, num_workers=cfg.num_workers)
    pseudo_items = generate_pseudo_labels(base, unlabeled_loader, cfg.confidence_threshold)
    print(f"Pseudo-labeled {len(pseudo_items)} images @ threshold {cfg.confidence_threshold}")

    train_loader, val_loader = combine_labeled_and_pseudo(dataset_dir, pseudo_items, cfg.max_unlabeled_ratio, cfg.batch_size, cfg.val_ratio, cfg.num_workers)

    model = create_resnet18(NUM_CLASSES, pretrained=False).to(DEVICE)
    model.load_state_dict(ckpt['model_state'])

    criterion = nn.CrossEntropyLoss()
    optimizer = create_optimizer(model, cfg.lr, cfg.weight_decay)
    scheduler = create_scheduler(optimizer, cfg.warmup_epochs, cfg.epochs_finetune)

    best_val_acc = 0.0
    for epoch in range(cfg.epochs_finetune):
        tr = train_one_epoch(model, train_loader, criterion, optimizer)
        val = evaluate(model, val_loader, criterion)
        scheduler.step()
        print(f"[SSL] Epoch {epoch+1}/{cfg.epochs_finetune} | train {tr} | val {val}")
        if val['acc'] > best_val_acc:
            best_val_acc = val['acc']
            torch.save({'model_state': model.state_dict(), 'class_map': ckpt.get('class_map', None)}, out_ckpt_path)
            print(f"Saved SSL checkpoint: {out_ckpt_path} (val_acc={best_val_acc:.4f})")
    return out_ckpt_path
# @title Phase 1: Train on Labeled Data and Export Predictions
PHASE1_EPOCHS = int(os.environ.get('PHASE1_EPOCHS', 15))
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 64))
VAL_RATIO = float(os.environ.get('VAL_RATIO', 0.1))
LR = float(os.environ.get('LR', 1e-3))
WEIGHT_DECAY = float(os.environ.get('WEIGHT_DECAY', 1e-4))
WARMUP_EPOCHS = int(os.environ.get('WARMUP_EPOCHS', 1))
NUM_WORKERS = int(os.environ.get('NUM_WORKERS', 2))

PHASE1_CKPT = str(Path(DATASET_ROOT) / 'phase1_resnet18.pth')

model, ckpt_path, loaders = train_model(DATASET_DIR, epochs=PHASE1_EPOCHS, batch_size=BATCH_SIZE, lr=LR, weight_decay=WEIGHT_DECAY, warmup_epochs=WARMUP_EPOCHS, val_ratio=VAL_RATIO, num_workers=NUM_WORKERS, checkpoint_path=PHASE1_CKPT)

ckpt = torch.load(ckpt_path, map_location=DEVICE)
model.load_state_dict(ckpt['model_state'])
model.eval()

_, _, _, _, test_loader = loaders
rows = []
with torch.no_grad():
    for images, fnames in test_loader:
        images = images.to(DEVICE, non_blocking=True)
        logits = model(images)
        preds = logits.argmax(dim=1).cpu().tolist()
        for fname, pred in zip(fnames, preds):
            rows.append({'path': fname, 'predicted_label': f'class_{pred}'})

phase1_csv = str(Path(DATASET_ROOT) / 'phase1_predictions.csv')
pd.DataFrame(rows).to_csv(phase1_csv, index=False)
print('Saved:', phase1_csv)
# @title Train/Eval Utilities
from collections import defaultdict

def evaluate(model: nn.Module, data_loader: DataLoader, criterion: nn.Module):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total_count = 0
    class_correct = defaultdict(int)
    class_count = defaultdict(int)
    with torch.no_grad():
        for images, labels in data_loader:
            images = images.to(DEVICE, non_blocking=True)
            labels = labels.to(DEVICE, non_blocking=True)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * labels.size(0)
            preds = outputs.argmax(dim=1)
            corr = (preds == labels)
            total_correct += corr.sum().item()
            total_count += labels.size(0)
            for l, c in zip(labels.cpu().tolist(), corr.cpu().tolist()):
                class_count[l] += 1
                if c:
                    class_correct[l] += 1
    return {
        'loss': total_loss / max(1,total_count),
        'acc': total_correct / max(1,total_count),
        'per_class_acc': {int(k): class_correct[k]/max(1,class_count[k]) for k in class_count}
    }


def train_one_epoch(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0.0
    total_correct = 0
    total_count = 0
    for images, labels in loader:
        images = images.to(DEVICE, non_blocking=True)
        labels = labels.to(DEVICE, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)
        logits = model(images)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * labels.size(0)
        preds = logits.argmax(dim=1)
        total_correct += (preds == labels).sum().item()
        total_count += labels.size(0)
    return {'loss': total_loss/max(1,total_count), 'acc': total_correct/max(1,total_count)}


def train_model(dataset_dir: str, epochs: int = 15, batch_size: int = 64, lr: float = 1e-3, weight_decay: float = 1e-4, warmup_epochs: int = 1, val_ratio: float = 0.1, num_workers: int = 2, checkpoint_path: str = 'best_resnet18.pth'):
    ds_full, train_loader, val_loader, unlabeled_loader, test_loader = build_dataloaders(dataset_dir, batch_size, val_ratio, num_workers)
    model = create_resnet18(NUM_CLASSES, pretrained=False).to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = create_optimizer(model, lr, weight_decay)
    scheduler = create_scheduler(optimizer, warmup_epochs, epochs)
    best_val_acc = 0.0
    for epoch in range(epochs):
        t0 = time.time()
        tr = train_one_epoch(model, train_loader, criterion, optimizer)
        val = evaluate(model, val_loader, criterion)
        scheduler.step()
        print(f"Epoch {epoch+1}/{epochs} | train {tr} | val {val} | {time.time()-t0:.1f}s")
        if val['acc'] > best_val_acc:
            best_val_acc = val['acc']
            torch.save({'model_state': model.state_dict(), 'class_map': getattr(ds_full, 'class_to_idx', None)}, checkpoint_path)
            print(f"Saved checkpoint: {checkpoint_path} (val_acc={best_val_acc:.4f})")
    return model, checkpoint_path, (train_loader, val_loader, unlabeled_loader, test_loader)
# @title Model, Optimizer, Scheduler

def create_resnet18(num_classes: int = NUM_CLASSES, pretrained: bool = False) -> nn.Module:
    model = torchvision.models.resnet18(weights=None if not pretrained else torchvision.models.ResNet18_Weights.DEFAULT)
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, num_classes)
    return model


def create_optimizer(model: nn.Module, lr: float = 1e-3, weight_decay: float = 1e-4):
    return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)


def create_scheduler(optimizer, warmup_epochs: int, total_epochs: int):
    def lr_lambda(current_epoch):
        if current_epoch < warmup_epochs:
            return float(current_epoch + 1) / float(max(1, warmup_epochs))
        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
        return 0.5 * (1.0 + np.cos(np.pi * progress))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
# @title Datasets and Dataloaders
from pathlib import Path

class ImageDatasetWithLabels(Dataset):
    def __init__(self, root: str, csv_path: str, transform=None):
        self.root = Path(root)
        self.transform = transform
        df = pd.read_csv(csv_path)
        self.paths = df['path'].astype(str).tolist()
        self.labels_str = df['label'].astype(str).tolist()
        classes = sorted(set(self.labels_str))
        self.class_to_idx = {c: i for i, c in enumerate(classes)}
        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}
        self.labels = [self.class_to_idx[s] for s in self.labels_str]
    def __len__(self):
        return len(self.paths)
    def __getitem__(self, idx: int):
        rel_path = self.paths[idx]
        img_path = self.root / 'images' / rel_path if (self.root / 'images').exists() else self.root / rel_path
        with Image.open(img_path).convert('RGB') as img:
            if self.transform is not None:
                img = self.transform(img)
        return img, self.labels[idx]

class ImageFolderNoLabels(Dataset):
    def __init__(self, root: str, transform=None):
        self.root = Path(root)
        self.files = sorted([p.name for p in self.root.glob('*.jpg')])
        self.transform = transform
    def __len__(self):
        return len(self.files)
    def __getitem__(self, idx: int):
        fname = self.files[idx]
        with Image.open(self.root / fname).convert('RGB') as img:
            if self.transform is not None:
                img = self.transform(img)
        return img, fname

IM_SIZE = 224
train_tfms = transforms.Compose([
    transforms.Resize((IM_SIZE, IM_SIZE)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.ColorJitter(0.2,0.2,0.2,0.02),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])
val_tfms = transforms.Compose([
    transforms.Resize((IM_SIZE, IM_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

def build_dataloaders(dataset_dir: str, batch_size: int = 64, val_ratio: float = 0.1, num_workers: int = 2):
    labeled_root = str(Path(dataset_dir) / 'labeled_data')
    labeled_csv = str(Path(labeled_root) / 'labels.csv')
    ds_full = ImageDatasetWithLabels(labeled_root, labeled_csv, transform=train_tfms)
    val_size = max(1, int(len(ds_full) * val_ratio))
    train_size = len(ds_full) - val_size
    ds_train, ds_val = random_split(ds_full, [train_size, val_size])
    class _ValWrapper(Dataset):
        def __init__(self, subset):
            self.subset = subset
            self.root = subset.dataset.root
            self.paths = [subset.dataset.paths[i] for i in subset.indices]
            self.labels = [subset.dataset.labels[i] for i in subset.indices]
        def __len__(self):
            return len(self.paths)
        def __getitem__(self, idx):
            rel_path = self.paths[idx]
            img_path = self.root / 'images' / rel_path if (self.root / 'images').exists() else self.root / rel_path
            with Image.open(img_path).convert('RGB') as img:
                img = val_tfms(img)
            return img, self.labels[idx]
    ds_val_wrapped = _ValWrapper(ds_val)

    unlabeled_root = str(Path(dataset_dir) / 'unlabeled_data')
    test_root = str(Path(dataset_dir) / 'test_images')

    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(ds_val_wrapped, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    unlabeled_loader = DataLoader(ImageFolderNoLabels(unlabeled_root, val_tfms), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(ImageFolderNoLabels(test_root, val_tfms), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    return ds_full, train_loader, val_loader, unlabeled_loader, test_loader
# @title Imports, Seeding, Utils
import random, time
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from PIL import Image

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision
from torchvision import transforms

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
NUM_CLASSES = 10


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
# @title Setup: Environment, Paths, Optional Drive Mount
import os, sys
from pathlib import Path

IN_COLAB = "google.colab" in sys.modules

if IN_COLAB:
    try:
        from google.colab import drive  # type: ignore
        drive.mount('/content/drive')
    except Exception as e:
        print('Drive mount skipped:', e)

DEFAULT_DATASET_ROOT = "/content" if IN_COLAB else "/workspace"
DATASET_ROOT = os.environ.get("DATASET_ROOT", DEFAULT_DATASET_ROOT)
DATASET_DIR = str(Path(DATASET_ROOT) / "dataset")
print("Using DATASET_DIR:", DATASET_DIR)
# HyperVerge Campus Placements â€” ResNet-18 Baseline and Semi-Supervised

Train a 10-class image classifier using only ResNet-18 in two phases:

- Phase 1: Train only on labeled data and export `phase1_predictions.csv` for `test_images/`.
- Phase 2: Semi-supervised finetuning via pseudo-labeling on unlabeled data and export `phase2_predictions.csv`.

Rules:
- Use only dataset images: `labeled_data/`, `unlabeled_data/`, `test_images/`.
- Use only ResNet-18 (no other architectures).
- No external datasets. (Pretrained weights disabled by default.)

Run cells top-to-bottom. Configure dataset root in the next cell if needed.